title,abstract,authors,url
A Graph Representation of Semi-structured Data for Web Question Answering ,"The abundant semi-structured data on the Web, such as HTML-based tables and lists, provide commercial search engines a rich information source for question answering (QA). Different from plain text passages in Web documents, Web tables and lists have inherent structures, which carry semantic correlations among various elements in tables and lists. Many existing studies treat tables and lists as flat documents with pieces of text and do not make good use of semantic information hidden in structures. In this paper, we propose a novel graph representation of Web tables and lists based on a systematic categorization of the components in semi-structured data as well as their relations. We also develop pre-training and reasoning techniques on the graph model for the QA task. Extensive experiments on several real datasets collected from a commercial engine verify the effectiveness of our approach. Our method improves F1 score by 3.90 points over the state-of-the-art baselines.","xingyao zhang, Linjun Shou, Jian Pei, Ming Gong, Lijie Wen and Daxin Jiang",http://arxiv.org/abs/2010.06801v1
A High Precision Pipeline for Financial Knowledge Graph Construction ,"Motivated by applications such as question answering, fact checking, and data integration, there is significant interest in constructing knowledge graphs by extracting information from unstructured information sources, particularly text documents.  Knowledge graphs have emerged as a standard for structured knowledge representation, whereby entities and their inter-relations are represented and  conveniently stored as (subject,predicate,object) triples in a graph that  can be used to power various downstream applications.  The proliferation of financial news sources reporting on companies, markets, currencies, and stocks presents an opportunity for extracting valuable knowledge about this crucial domain. In this paper, we focus on constructing a knowledge graph automatically by information extraction from a large corpus of financial news articles. For that purpose, we develop a high precision knowledge extraction pipeline tailored for the financial domain. This pipeline combines multiple information extraction techniques with a financial dictionary that we built, all working together to produce over 342,000 compact extractions from over 288,000 financial news articles, with a precision of 78% at the top-100 extractions.The extracted triples are stored in a knowledge graph making them readily available for use in downstream applications.","Sarah Elhammadi, Laks V.S. Lakshmanan, Raymond Ng, Michael Simpson, Baoxing Huai, Zhefeng Wang and Lanjun Wang",http://arxiv.org/abs/physics/0701061v1
"A Study on Efficiency, Accuracy and Document Structure for Answer Sentence Selection ","An essential task of most Question Answering (QA) systems is to re-rank the set of answer candidates, i.e., Answer Sentence Selection (A2S). These candidates are typically sentences either extracted from one or more documents preserving their natural order or retrieved by a search engine. Most state-of-the-art approaches to the task use huge neural models, such as BERT, or complex attentive architectures.
In this paper, we argue that by exploiting the intrinsic structure of the original rank together with an effective word-relatedness encoder, we can achieve competitive results with respect to the state of the art while retaining high efficiency. Our model takes 9.5 seconds to train on the WikiQA dataset, i.e., very fast in comparison with the 18 minutes required by a standard BERT-base fine-tuning.",Daniele Bonadiman and Alessandro Moschitti,http://arxiv.org/abs/2003.02349v1
A Vietnamese Dataset for Evaluating Machine Reading Comprehension ,"Over 97 million inhabitants speak Vietnamese as the native language in the world. However, there are few research studies on machine reading comprehension (MRC) in Vietnamese, the task of understanding a document or text, and answering questions related to it. Due to the lack of benchmark datasets for Vietnamese, we present the Vietnamese Question Answering Dataset (ViQuAD), a new dataset for the low-resource language as Vietnamese to evaluate MRC models. This dataset comprises over 23,000 human-generated question-answer pairs based on 5,109 passages of 174 Vietnamese articles from Wikipedia. In particular, we propose a new process of dataset creation for Vietnamese MRC. Our in-depth analyses illustrate that our dataset requires abilities beyond simple reasoning like word matching and demands complicate reasoning such as single-sentence and multiple-sentence inferences. Besides, we conduct experiments on state-of-the-art MRC methods in English and Chinese as the first experimental models on ViQuAD, which will be compared to further models. We also estimate human performances on the dataset and compare it to the experimental results of several powerful machine models. As a result, the substantial differences between humans and the best model performances on the dataset indicate that improvements can be explored on ViQuAD through future research. Our dataset is freely available to encourage the research community to overcome challenges in Vietnamese MRC.","Kiet Nguyen, Vu Nguyen, Anh Nguyen and Ngan Nguyen",http://arxiv.org/abs/2009.14725v3
An empirical analysis of existing systems and datasets toward general simple question answering ,"In this paper, we evaluate the progress of our field toward solving simple factoid questions over a knowledge base, a practically important problem in natural language interface to database.
As in other natural language understanding tasks, a common practice for this task is to train and evaluate a model on a single dataset, and recent studies suggest that SimpleQuestions, the most popular and largest dataset, is nearly solved under this setting.
However, this common setting does not evaluate the robustness of the systems outside of the distribution of the used training data.
We rigorously evaluate such robustness of existing systems using different datasets.
Our analysis, including shifting of train and test datasets and training on a union of the datasets, suggests that our progress in solving SimpleQuestions dataset does not indicate the success of more general simple question answering.
We discuss a possible future direction toward this goal.","Namgi Han, Goran Topic, Hiroshi Noji, Hiroya Takamura and Yusuke Miyao",http://arxiv.org/abs/1703.09684v2
Answer-driven Deep Question Generation based on Reinforcement Learning ,"Deep question generation (DQG) aims to generate complex questions through reasoning over
multiple documents. The task is challenging and underexplored. Existing methods mainly focus on enhancing document representations, with little attention paid to the answer information, which may result in the generated question not matching the answer type and being answerirrelevant. In this paper, we propose an Answer-driven Deep Question Generation (ADDQG) model based on the encoder-decoder framework. The model makes better use of the target answer as a guidance to facilitate question generation. First, we propose an answer-aware initialization module with a gated connection layer which introduces both document and answer information to the decoder, thus helping to guide the choice of answer-focused question words. Then a semantic-rich fusion attention mechanism is designed to support the decoding process, which integrates the answer with the document representations to promote the proper handling of answer information during generation. Moreover, reinforcement learning is applied to integrate both syntactic and semantic metrics as the reward to enhance the training of the ADDQG. Extensive experiments on the HotpotQA dataset show that ADDQG outperforms state-of-the-art models in both automatic and human evaluations.","Liuyin Wang, Zihan Xu, Zibo Lin, Haitao Zheng and Ying Shen",http://arxiv.org/abs/1901.02219v1
Answering Legal Questions by Learning Neural Attentive Text Representation ,"Text representation plays a vital role in retrieval-based question answering, especially in the legal domain where documents are usually long and complicated. The better the question and the legal documents are represented, the more accurate they are matched. In this paper, we focus on the task of answering legal questions at the article level. Given a legal question, the goal is to retrieve all the correct and valid legal articles, that can be used as the basic to answer the question. We present a retrieval-based model for the task by learning neural attentive text representation. Our text representation method first leverages convolutional neural networks to extract important information in a question and legal articles. Attention mechanisms are then used to represent the question and articles and select appropriate information to align them in a matching process. Experimental results on an annotated corpus consisting of 5,922 Vietnamese legal questions show that our model outperforms state-of-the-art retrieval-based methods for question answering by large margins in terms of both recall and NDCG.","Phi Manh Kien, Nguyen Ha Thanh, Ngo Xuan Bach, Vu Tran, Nguyen Le Minh and Tu Minh Phuong",http://arxiv.org/abs/1705.08432v2
Ask to Learn: A Study on Curiosity-driven Question Generation ,"We propose a novel text generation task, namely Curiosity-driven Question Generation. We start from the observation that the Question Generation task has traditionally been considered as the dual problem of Question Answering, hence tackling the problem of generating a question given the text that contains its answer. Such questions can be used to evaluate machine reading comprehension. However, in real life, and especially in conversational settings, humans tend to ask questions with the goal of enriching their knowledge and/or clarifying aspects of previously gathered information. ",Thomas Scialom and Jacopo Staiano,http://arxiv.org/abs/1911.03350v1
Automated Graph Generation at Sentence Level for Reading Comprehension Based on Conceptual Graphs ,"This paper proposes a novel miscellaneous-context-based method to convert a sentence into a knowledge embedding in the form of a directed graph.  We adopt the idea of conceptual graphs to frame for the miscellaneous textual information into conceptual compactness. We first empirically observe that this graph representation method can (1) accommodate the slot-filling challenges in typical question answering and (2) access to the sentence-level graph structure in order to explicitly capture the neighbouring connections of reference concept nodes.   Secondly,  we propose a task-agnostic semantics-measured module, which cooperates with the graph representation method, in order to (3) project an edge of a sentence-level graph to the space of semantic relevance with respect to the corresponding concept nodes.  As a result of question-answering experiments, the combination of the graph representation and the semantics-measured module achieves the high accuracy of answer prediction and offers human-comprehensible graphical interpretation for every well-formed sample.  To our knowledge, our approach is the first towards the interpretable process of learning vocabulary representations with the experimental evidence.",Wan-Hsuan Lin and Chun-Shien Lu,http://arxiv.org/abs/1803.03664v1
Bi-directional CognitiveThinking Network for Machine Reading Comprehension ,"We propose a novel Bi-directional Cognitive Knowledge Framework (BCKF) for reading comprehension from the perspective of complementary learning systems theory. It aims to simulate two ways of thinking in the brain to answer questions, including reverse thinking and inertial thinking. To validate the effectiveness of our framework, we design a corresponding Bi-directional Cognitive Thinking Network (BCTN) to encode the passage and generate a question (answer) given an answer (question) and decouple the bi-directional knowledge. The model has the ability to reverse reasoning questions which can assist inertial thinking to generate more accurate answers. Competitive improvement is observed in DuReader dataset, confirming our hypothesis that bi-directional knowledge helps the QA task. The novel framework shows an interesting perspective on machine reading comprehension and cognitive science.","Wei Peng, Yue Hu, Luxi Xing, Yuqiang Xie, Jing Yu, Yajing Sun and Xiangpeng Wei",http://arxiv.org/abs/1607.02250v3
BioMedBERT: A Pre-trained Biomedical Language Model for QA and IR ,"The SARS-CoV-2 (COVID-19) pandemic spotlighted the importance of moving quickly with biomedical research. However, as the number of biomedical research papers continue to increase, the task of finding relevant articles to answer pressing questions has become significant. In this work, we propose a textual data mining tool that supports literature search to accelerate the work of researchers in the biomedical domain. We achieve this by building a neural-based deep contextual understanding model for Question-Answering (QA) and Information Retrieval (IR) tasks. We also leverage the new BREATHE dataset which is one of the largest available datasets of biomedical research literature, containing abstracts and full-text articles from ten different biomedical literature sources on which we pre-train our BioMedBERT model. Our work achieves state-of-the-art results on the QA fine-tuning task on BioASQ 5b, 6b and 7b datasets. In addition, we observe superior relevant results when BioMedBERT embeddings are used with Elasticsearch for the Information Retrieval task on the intelligently formulated BioASQ dataset. We believe our diverse dataset and our unique model architecture are what led us to achieve the state-of-the-art results for QA and IR tasks.","SOURADIP CHAKRABORTY, Ekaba Bisong, Shweta Bhatt, Thomas Wagner, Riley Elliott and Francesco Mosconi",
CharBERT: Character-aware Pre-trained Language Model ,"Pre-trained language models (PLMs) have achieved great progress in various language understanding benchmarks. Most of the previous works construct the representations in the subword level by Byte-Pair Encoding (BPE) or its variations, which make the word representation incomplete and fragile. In this paper, we propose a character-aware pre-trained language model named CharBERT, improving on the previous methods (such as BERT, RoBERTa) to tackle the problem. We first construct the contextual word embedding for each token from the sequential character representations, and fuse the representations from character and subword iteratively by a heterogeneous interaction module. Then we propose a new pre-training task for unsupervised character learning. We evaluate the method on question answering, sequence labeling, and text classification tasks, both on the original dataset and adversarial misspelling test set. The experimental results show that our method can significantly improve performance and robustness.","Wentao Ma, Yiming Cui, Chenglei Si, Ting Liu, Shijin Wang and Guoping Hu",
CHIME: Cross-passage Hierarchical Memory Network for Generative Review Question Answering ,"We introduce CHIME, a cross-passage hierarchical memory network for question answering (QA) via text generation. It extends XLNet introducing an auxiliary memory module consisting of two components: the context memory collecting cross-passage evidences, and the answer memory working as a buffer continually refining the generated answers. Empirically, we show the efficacy of the proposed architecture in the multi-passage generative QA, outperforming the state-of-the-art baselines with better syntactically well-formed answers and increased precision in addressing the questions of the AmazonQA review dataset. An additional qualitative analysis reveals the rationale of the underlying generative process.","Junru Lu, Gabriele Pergola, Lin Gui, Binyang Li and Yulan He",
Commonsense Question Answering Boosted by Graph-Based Iterative Retrieval over Multiple Knowledge Sources ,"To better understand natural language text and speech, it is critically required to make use of background or commonsense knowledge. However, how to efficiently leverage external knowledge in question-answering systems is still a hot research topic in both academic and industrial communities. In this paper, we propose a novel question-answering method with integrating multiple knowledge sources. More specifically, we first introduce a novel graph-based iterative knowledge acquisition module with potential relations to retrieve both concepts and entities related to the given question. After obtaining the relevant knowledge, we utilize a pre-trained language model to encode the question with its evidence and present a question-aware attention mechanism to fuse all representations by previous modules. At last, a task-specific linear classifier is used to predict the possibility. We conduct experiments on the dataset, CommonsenseQA, and the results show that our proposed method outperforms other competitive methods and archives a new state-of-the-art. Furthermore, we also conduct ablation studies to demonstrate the effectiveness of our proposed graph-based iterative knowledge acquisition module and question-aware attention module and find the key properties that are helpful to the method.","Qianglong Chen, Feng Ji, Haiqing Chen and Yin Zhang",http://arxiv.org/abs/2011.02705v1
Constructing A Multi-hop QA Dataset for Comprehensive Evaluation of Reasoning Steps ,"A multi-hop dataset aims to test reasoning and inference skills by requiring a model to read multiple paragraphs to answer a given question.
However, current datasets do not provide a complete explanation for the reasoning process from the question to the answer.
Further, previous studies revealed that many examples in existing multi-hop datasets do not require multi-hop reasoning to answer a question.
In this study, we present a new multi-hop dataset, called 2WikiMultiHopQA, by using Wikipedia and Wikidata.
In our dataset, we introduced the evidence information containing a reasoning path for multi-hop questions.
The evidence information has two benefits: (i) providing a comprehensive explanation for predictions and (ii) evaluating the reasoning skills of a model.
We carefully designed a pipeline and a set of templates when generating a question--answer pair that guarantees the multi-hop steps and the quality of the questions.
We also exploited the structured format in Wikidata and use logical rules to create questions that are natural but still require multi-hop reasoning.
Through experiments, we demonstrated that our dataset is challenging for multi-hop models and it ensures that multi-hop reasoning is required.","Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara and Akiko Aizawa",http://arxiv.org/abs/2011.01060v2
Conversational Machine Comprehension: a Literature Review ,"Conversational Machine Comprehension (CMC) is a research track in conversational AI which expects the machine to understand an open-domain text and thereafter engage in a multi-turn conversation to answer questions related to the text. While most of the research in Machine Reading Comprehension (MRC) revolves around single-turn question answering (QA), multi-turn CMC has recently gained prominence, thanks to the advancement in natural language understanding via neural language models such as BERT and the introduction of large-scale conversational datasets such as CoQA and QuAC. The rise in interest has, however, led to a flurry of concurrent publications, each with a different yet structurally similar modeling approach and an inconsistent view of the surrounding literature. With the volume of model submissions to conversational datasets increasing every year, there exists a need to consolidate the scattered knowledge in this domain to streamline future research. This literature review attempts at providing a holistic overview of CMC with an emphasis on the common trends across recently published models, specifically in their approach to tackling conversational history. The review synthesizes a generic framework for CMC models while highlighting the differences in recent approaches and intends to serve as a compendium of CMC for future researchers.","Somil Gupta, Bhanu Pratap Singh Rawat and hong yu",http://arxiv.org/abs/2006.00671v2
CosMo: Conditional Seq2Seq-based Mixture Model for Zero-Shot Commonsense Question Answering ,"Commonsense reasoning refers to the ability of evaluating a social situation and acting accordingly. Identification of the implicit causes and effects of a social context is the driving capability which can enable machines to perform commonsense reasoning. The dynamic world of social interactions requires context-dependent on-demand systems to infer such underlying information. However, current approaches in this realm lack the ability to perform commonsense reasoning upon facing an unseen situation, mostly due to incapability of identifying a diverse range of implicit social relations. Hence they fail to estimate the correct reasoning path.
In this paper, we present Conditional Seq2Seq-based Mixture model (CosMo), which provides us with the capabilities of dynamic and diverse content generation. We use CosMo to generate context-dependent clauses, which form a dynamic Knowledge Graph (KG) on-the-fly for commonsense reasoning.
To show the adaptability of our model to context-dependant knowledge generation, we address the task of zero-shot commonsense question answering. The empirical results indicate an improvement of up to +5.2\% over the state-of-the-art models.","Farhad Moghimifar, Lizhen Qu, Yue Zhuo, Mahsa Baktashmotlagh and Gholamreza Haffari",
Diverse and Non-redundant Answer Set Extraction on Community QA based on DPPs ,"In community-based question answering (CQA) platforms, the problem is that it takes time to get useful information from many answers to a question.
One of the solutions, ranking methods, had the problem of showing similar answers at the top, or the importance of the answers could only be determined by their similarity to the question or the best answer.
Therefore, we propose a new task of selecting a diverse and non-redundant answer set, rather than ranking the answers.
We build a dataset for the task and propose a solution using Determinantal Point Processes (DPPs), probabilistic models that give higher probability mass to more high-quality and diverse subsets, and BERT, which has achieved high accuracy in many tasks recently.
The proposed methods outperformed several baseline methods in the task.","Shogo Fujita, Tomohide Shibata and Manabu Okumura",http://arxiv.org/abs/2011.09140v1
Dual Supervision Framework for Relation Extraction with Distant Supervision and Human Annotation ,"Relation extraction (RE) has been extensively studied due to its importance in real-world applications such as knowledge base construction and question answering. Most of the existing works train the models on either distantly supervised data or human-annotated data. To take advantage of the high accuracy of human annotation and the cheap cost of distant supervision, we propose the dual supervision framework which effectively utilizes both types of data. However, simply combining the two types of data to train a RE model may decrease the prediction accuracy since distant supervision has labeling bias. We employ two separate prediction networks HA-Net and DS-Net to predict the labels by human annotation and distant supervision, respectively, to prevent the degradation of accuracy by the incorrect labeling of distant supervision. Furthermore, we propose an additional loss term called disagreement penalty to enable HA-Net to learn from distantly supervised labels. In addition, we exploit additional networks to adaptively assess the labeling bias by considering contextual information. Our performance study on sentence-level and document-level REs confirms the effectiveness of the dual supervision framework.",Woohwan Jung and Kyuseok Shim,http://arxiv.org/abs/2011.11851v1
Explain by Evidence: An Explainable Memory-based Neural Network for Question Answering ,"Interpretability and explainability of deep neural net models are always challenging due to their size and complexity. Many previous works focused on visualizing internal components of neural networks to represent them through human-friendly concepts. On the other hand, in real life, when making a decision, human tends to rely on similar situations in the past. Thus, we argue that one potential approach to make the model interpretable and explainable is to design it in a way such that the model explicitly connects the current sample with the seen samples, and bases its decision on these samples. In this work, we design one such model: an explainable, evidence-based memory network architecture, which learns to summarize the dataset and extract supporting evidences to make its decision. The model achieves state-of-the-art performance on two popular question answering datasets, the TrecQA dataset and the WikiQA dataset. Via further analysis, we showed that this model can reliably trace the errors it has made in the validation step to the training instances that might have caused this error. We believe that this error-tracing capability might be beneficial in improving dataset quality in many applications.","Quan Hung Tran, Nhan Dam, Tuan Lai, Franck Dernoncourt, Trung Le, Nham Le and Dinh Phung",http://arxiv.org/abs/1901.06560v1
FASTMATCH: Accelerating the Inference of BERT-based Text Matching ,"Recently, pre-trained language models such as BERT have shown state-of-the-art accuracies in
text matching. When being applied to IR (or QA), the BERT-based matching models need to
online calculate the representations and interactions for all query-candidate pairs. The high inference
cost has prohibited the deployments of BERT-based matching models in many practical
applications. To address this issue, we propose a novel BERT-based text matching model, in
which the representations and the interactions are decoupled. Then, the representations of the
candidates can be calculated and stored offline, and directly retrieved during the online matching
phase. To conduct the interactions and generate final matching scores, a lightweight attention network
is designed. Experiments based on several large scale text matching datasets show that the
proposed model, called FASTMATCH, can achieve up to 100X speed-up to BERT and RoBERTa
at the online matching phase, while keeping more up to 98:7% of the performance.","Shuai Pang, Jianqiang Ma, ZEYU YAN, Yang Zhang and Jianping Shen",
Finding the Evidence: Localization-aware Answer Prediction for Text Visual Question Answering ,"Image text carries essential information to understand the scene and perform reasoning. Text-based visual question answering (text VQA) task focuses on visual questions that require reading text in images.
Existing text VQA systems generate an answer by selecting from optical character recognition (OCR) texts or a fixed vocabulary. Positional information of text is underused and there is a lack of evidence for the generated answer. As such, this paper proposes a localization-aware answer prediction network (LaAP-Net) to address this challenge. Our LaAP-Net not only generates the answer to the question but also predicts a bounding box as evidence of the generated answer. Moreover, a context-enriched OCR representation (COR) for multimodal fusion is proposed to facilitate the localization task. Our proposed LaAP-Net outperforms existing approaches on three benchmark datasets for the text VQA task by a noticeable margin.","Wei Han, Hantao Huang and Tao Han",http://arxiv.org/abs/2010.02582v1
Handling Anomalies of Synthetic Questions in Unsupervised Question Answering ,"Advances in Question Answering (QA) research require additional datasets for new domains, languages, types of questions, as well as for performance increases. Human creation of a QA dataset like SQuAD, however, is expensive. As an alternative, unsupervised QA approach has been proposed so that QA training data can be generated automatically. However, their QA performance is much lower than that of supervised QA models. We identify two anomalies in the generated questions and propose methods for mitigating them. We show that our approach significantly improves the performance of unsupervised QA across a number of QA tasks.","Giwon Hong, Junmo Kang, Doyeon Lim and Sung-Hyon Myaeng",http://arxiv.org/abs/1607.04354v3
I Know What You Asked: Graph Path Learning using AMR for Commonsense Reasoning ,"CommonsenseQA is a task in which a correct answer is predicted through commonsense reasoning with pre-defined knowledge. Most previous works have aimed to improve its performance with distributed representation without considering the process of predicting the answer from the semantic representation of the question. To shed light upon the semantic interpretation of the question, we propose the AMR-ConceptNet-Pruned (ACP) graph. The ACP graph is pruned from a full integrated graph encompassing Abstract Meaning Representation (AMR) graph generated from input questions and an external commonsense knowledge graph, ConceptNet (CN). Then the ACP graph is exploited to interpret the reasoning path as well as to predict the correct answer on the CommonsenseQA task. To demonstrate the interpretability and effectiveness of the graph, this paper presents the manner in which the commonsense reasoning process can be interpreted with the relations and concepts provided by the ACP graph. Moreover, ACP-based models are shown to outperform the baselines.","Jungwoo Lim, Dongsuk Oh, Yoonna Jang, Kisu Yang and Heuiseok Lim",http://arxiv.org/abs/2011.00766v2
Improving Conversational Question Answering Systems after Deployment using Feedback-Weighted Learning ,"The interaction of conversational systems with users poses an exciting opportunity for improving them after deployment, but little evidence has been provided of its feasibility. 
In most applications, users are not able to provide the correct answer to the system, but they are able to provide binary (correct, incorrect) feedback. In this paper we propose feedback-weighted learning based on importance sampling to improve upon an initial supervised system using binary user feedback. We perform simulated experiments on document classification (for development) and Conversational Question Answering datasets like QuAC and DoQA, where binary user feedback is derived from gold annotations. The results show that our method is able to improve over the initial supervised system, getting close to a fully-supervised system that has access to the same labeled examples in in-domain experiments (QuAC), and even matching in out-of-domain experiments (DoQA).  Our work opens the prospect to exploit interactions with real users and improve conversational systems after deployment.","Jon Ander Campos, Arantxa Otegi, Aitor Soroa, Kyunghyun Cho, Eneko Agirre and Gorka Azkune",http://arxiv.org/abs/2011.00615v1
IntKB: A Verifiable Interactive Framework for Knowledge Base Completion ,"Knowledge bases (KBs) are essential for many downstream NLP tasks, yet their prime shortcoming is that they are often incomplete. State-of-the-art frameworks for KB completion often lack sufficient accuracy to work fully automated without human supervision. As a remedy, we propose \intkb: a novel interactive framework for KB completion from text based on a question answering pipeline. Our framework is tailored to the specific needs of a human-in-the-loop paradigm: (i) We generate facts that are aligned with text snippets and are thus immediately verifiable by humans. (ii) Our system is designed such that it continuously learns during the KB completion task and, therefore, significantly improves its performance upon initial zero- and few-shot relations over time. (iii) We only trigger human interactions when there is enough information for a correct prediction. Therefore, we train our system with negative examples and a fold-option if there is no answer. Our framework yields a favorable performance: it achieves a hit@1 ratio of 26.3% for initially unseen relations, upon which it gradually improves to 45.7%.","Bernhard Kratzwald, Guo Kunpeng, Stefan Feuerriegel and Dennis Diefenbach",
Medical Knowledge-enriched Textual Entailment Framework ,"One of the cardinal tasks in achieving robust medical question answering systems is textual entailment.  The existing approaches make use of an ensemble of pre-trained language models or data augmentation, often to clock higher numbers on the validation metrics. However, two major shortcomings impede higher success in identifying entailment: (1) understanding the focus/intent of the question and (2) ability to utilize the real-world background knowledge to capture the con-text beyond the sentence. In this paper, we present a novel Medical Knowledge-Enriched Textual Entailment framework that allows the model to acquire a semantic and global representation of the input medical text with the help of a relevant domain-specific knowledge graph. We evaluate our framework on the benchmark MEDIQA-RQE dataset and manifest that the use of knowledge-enriched dual-encoding mechanism help in achieving an absolute improvement of 8.27% over SOTA language models.","Shweta Yadav, Vishal Pallagani and Amit Sheth",http://arxiv.org/abs/2011.05257v1
Modelling Long-distance Node Relations for KBQA with Global Dynamic Graph ,"The structural information of Knowledge Bases (KBs) has proven effective to Question Answering (QA). Previous studies rely on deep graph neural networks (GNNs) to capture rich structural information, which may not model node relations in particularly long distance due to oversmoothing issue. To address this challenge, we propose a novel framework \textbf{GlobalGraph}, which models long-distance node relations from two views: 1) Node type similarity: GlobalGraph assigns each node a global type label and models long-distance node relations through the global type label similarity; 2) Correlation between nodes and questions: we learn similarity scores between nodes and the question, and model long-distance node relations through the sum score of two nodes. We conduct extensive experiments on two widely used multi-hop KBQA datasets to prove the effectiveness of our method.","Xu Wang, Shuai Zhao, Jiale Han, Bo Cheng, Hao Yang, Jianchang Ao and Zhenzi Li",http://arxiv.org/abs/1912.05728v1
NUT-RC: Noisy User-generated Text-oriented Reading Comprehension ,"Reading comprehension (RC) on social media such as Twitter is a critical and challenging task due to its noisy, informal, but informative nature. Most existing RC models are developed on formal datasets such as news articles and Wikipedia documents, which severely limit their performances when directly applied to the noisy and informal texts in social media. Moreover, these models only focus on a certain type of RC, extractive or generative, but ignore the integration of them. To well address these challenges, we come up with a noisy user-generated text-oriented RC model. In particular, we first introduce a set of text normalizers to transform the noisy and informal texts to the formal ones. Then, we integrate the extractive and the generative RC model by a multi-task learning mechanism and an answer selection module. Experimental results on TweetQA demonstrate that our NUT-RC model significantly outperforms the state-of-the-art social media-oriented RC models.","Rongtao Huang, Bowei Zou, Yu Hong, Wei Zhang, AiTi Aw and Guodong Zhou",
QANom: Question-Answer driven SRL for Nominalizations ,"We propose a new semantic scheme for capturing predicate-argument relations for nominalizations, termed QANom. This scheme extends the QA-SRL formalism (He et al., 2015), modeling the relations between nominalizations and their arguments via natural language question-answer pairs. We construct the first QANom dataset using controlled crowdsourcing, analyze its quality and compare it to expertly annotated nominal-SRL annotations, as well as to other QA-driven annotations. In addition, we train a baseline QANom parser for identifying nominalizations and labeling their arguments with question-answer pairs. Finally, we demonstrate the extrinsic utility of our annotations for downstream tasks using both indirect supervision and zero-shot settings.","Ayal Klein, Jonathan Mamou, Valentina Pyatkin, Daniela Stepanov, Hangfeng He, Dan Roth, Luke Zettlemoyer and Ido Dagan",
Reinforced Multi-task Approach for Multi-hop Question Generation ,"Question generation (QG) attempts to solve the inverse of question answering (QA) problem by generating a natural language question given a document and an answer. While sequence to sequence neural models surpass rule-based systems for QG, they are limited in their capacity to focus on more than one supporting fact. For QG, we often require multiple supporting facts to generate high-quality questions. Inspired by recent works on multi-hop reasoning in QA, we take up Multi-hop question generation, which aims at generating relevant questions based on supporting facts in the context. We employ multitask learning with the auxiliary task of answer-aware supporting fact prediction to guide the question generator. In addition, we also proposed a question-aware reward function in a Reinforcement Learning (RL) framework to maximize the utilization of the supporting facts. We demonstrate the effectiveness of our approach through experiments on the multi-hop question answering dataset, HotPotQA. Empirical evaluation shows our model to outperform the single-hop neural question generation models on both automatic evaluation metrics such as BLEU, METEOR, and ROUGE and human evaluation metrics for quality and coverage of the generated questions.","Deepak Gupta, Hardik Chauhan, Ravi Tej Akella, Asif Ekbal and Pushpak Bhattacharyya",http://arxiv.org/abs/2005.14419v2
Robust Machine Reading Comprehension by Learning Soft labels ,"Neural models have achieved great success on the task of machine reading comprehension (MRC), which are typically trained on hard labels. We argue that hard labels limit the model capability on generalization due to the label sparseness problem. In this paper, we propose a robust training method for MRC models to address this problem. Our method consists of three strategies, 1) label smoothing, 2) word overlapping, 3) distribution prediction. All of them help to train models on soft labels. We validate our approach on the representative architecture - ALBERT. Experimental results show that our method can greatly boost the baseline with 1% improvement in average, and achieve state-of-the-art performance on NewsQA and QUOREF.","Zhenyu Zhao, Shuangzhi Wu, Muyun Yang, Kehai Chen and Tiejun Zhao",http://arxiv.org/abs/1511.04690v2
Schema Aware Semantic Reasoning for Interpreting Natural LanguageQueries in Enterprise Settings ,"Natural Language Query interfaces allow the end-users to access the desired information without the need to know any specialized query language, data storage, or schema details. Even with the recent advances in NLP research space, the state-of-the-art QA systems fall short of understanding implicit intents of real-world Business Intelligence (BI) queries in enterprise systems, since Natural Language Understanding still remains an AI-hard problem. We posit that deploying ontology reasoning over domain semantics can help in achieving better natural language understanding for QA systems. In this paper, we specifically focus on building a Schema Aware Semantic Reasoning Framework that translates natural language interpretation as a sequence of solvable tasks by an ontology reasoner. We apply our framework on top of an ontology based, state-of-the-art natural language question-answering system ATHENA, and experiment with $4$ benchmarks focused on BI queries. Our experimental numbers empirically show that the Schema Aware Semantic Reasoning indeed helps in achieving significantly better results for handling BI queries with an average accuracy improvement of ~30%","Jaydeep Sen, Tanaya Babtiwale, Kanishk Saxena, Yash Butala, Sumit Bhatia and Karthik Sankaranarayanan",http://arxiv.org/abs/1604.06361v1
Spotting Text-to-Text Patterns for Multiple-Choice Question Answering ,"While internalized implicit “knowledge base” in pre-trained models brings fruitful progress tonatural language understanding tasks, identifying an effective way to retrieve the knowledge fromthe model remains not fully explored. Based on the state-of-the-art unified text-to-text transfertransformer (T5) model, this work explores an approach to extract implicit knowledge on multiple-choice (MC) question answering tasks.  Through our experiments on three representative MCdatasets, we find that our text-to-text template approach performs surprisingly well. Furthermore,we verify that the proposed template can be easily extended to other MC tasks with an extraknowledge base. Starting from the MC task, this work initiates further research to find genericnatural language patterns that can effectively extract the stored knowledge in the pre-trainedmodels under the text-to-text paradigm.","Jheng-Hong Yang, Sheng-Chieh Lin, Rodrigo Nogueira, Ming-Feng Tsai, Chuan-Ju Wang and Jimmy Lin",http://arxiv.org/abs/1806.10866v1
The Devil is in the Details: Evaluating Limitations of Transformer-based Methods for Granular Tasks ,"Contextual embeddings derived from transformer-based neural language models have shown state-of-the-art performance for various tasks such as question answering, sentiment analysis, and textual similarity in recent years. Extensive work shows how accurately such models can represent abstract, semantic information present in text. In this expository work, we explore a tangent direction and analyze such models' performance on tasks that require a more granular level of representation.  We focus on the problem of textual similarity from two perspectives: matching documents on a granular level (requiring embeddings to capture fine-grained attributes in the text), and an abstract level (requiring  embeddings to capture overall textual semantics). We empirically demonstrate, across two datasets from different domains, that despite high performance in abstract document matching as expected, contextual embeddings are consistently (and at times, vastly) outperformed by simple baselines like TF-IDF for more granular tasks. We then propose a simple but effective method to incorporate TF-IDF into models that use contextual embeddings, achieving relative improvements of up to 36% on granular tasks.","Brihi Joshi, Leonardo Neves, Neil Shah and Francesco Barbieri",http://arxiv.org/abs/2011.01196v1
Towards Knowledge-Augmented Visual Question Answering ,"Visual Question Answering (VQA) remains algorithmically challenging while it is effortless for humans. Humans combine visual observations with general and commonsense knowledge to answer a question about a given image. In this paper, we address the problem of incorporating general knowledge into VQA models while leveraging the visual information. We propose a model that captures the interactions between objects in a visual scene and entities in an external knowledge source. Our model is a graph-based approach that combines scene graphs with entity graphs, which learns a question-adaptive graph representation of related knowledge instances. We use Graph Attention Networks to set higher importance to key knowledge instances that are mostly relevant to each question. We exploit ConceptNet as the source of general knowledge and evaluate the performance of our model on the challenging OK-VQA dataset.",Maryam Ziaeefard and Freddy Lecue,http://arxiv.org/abs/1410.8027v3
Understanding Unnatural Questions Improves Reasoning over Text ,"Complex question answering (CQA) over raw text is a challenging task.  A prominent approach to this task is based on the programmer-interpreter framework, where the programmer maps the question into a sequence of reasoning actions which is then executed on the raw text by the interpreter.  Learning an effective CQA model requires large amounts of human-annotated data, consisting of the ground-truth sequence of reasoning actions, which is time-consuming and ex-pensive to collect at scale.   In this paper, we address the challenge of learning a high-quality programmer (parser) by projecting natural human-generated questions into unnatural machine-generated questions which are more convenient to parse. We firstly generate synthetic (question, action sequence) pairs by a data generator, and train a semantic parser that associates synthetic questions with their corresponding action sequences.  To capture the diversity when applied to natural questions, we learn a projection model to map natural questions into their most similar unnatural questions for which the parser can work well.  Without any natural training data, our projection model provides high-quality action sequences for the CQA task.  Experimental results show that the QA model trained exclusively with synthetic data generated by our method outperforms its state-of-the-art counterpart trained on human-labeled data.","Xiaoyu Guo, Yuan-Fang Li and Gholamreza Haffari",http://arxiv.org/abs/2010.09366v1
